#!/usr/bin/env python3
"""
DeepSeek + æœ¬åœ°åµŒå…¥æ¨¡å‹ - å®Œæ•´RAGç³»ç»Ÿ
ä½¿ç”¨åŸç”ŸOpenAIå®¢æˆ·ç«¯è°ƒç”¨DeepSeek API
"""

import os
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def test_deepseek_api():
    """æµ‹è¯•DeepSeek APIè¿æ¥"""
    print("ğŸ§ª æµ‹è¯•DeepSeek APIè¿æ¥...")
    
    api_key = os.getenv('DEEPSEEK_API_KEY')
    if not api_key:
        print("âŒ æœªæ‰¾åˆ°DEEPSEEK_API_KEY")
        return False
    
    try:
        client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com"
        )
        
        response = client.chat.completions.create(
            model="deepseek-reasoner",  # DeepSeek-R1 æ¨ç†æ¨¡å‹
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªé£Ÿå“ç§‘å­¦åŠ©æ‰‹"},
                {"role": "user", "content": "ç®€å•ä»‹ç»ä¸€ä¸‹æŠ—æ°§åŒ–å‰‚"}
            ],
            max_tokens=100
        )
        
        print("âœ… DeepSeek APIè¿æ¥æˆåŠŸï¼")
        print(f"ğŸ“ æµ‹è¯•å›ç­”: {response.choices[0].message.content[:100]}...")
        return True
        
    except Exception as e:
        print(f"âŒ DeepSeek APIæµ‹è¯•å¤±è´¥: {e}")
        return False

def build_rag_with_deepseek():
    """ä½¿ç”¨DeepSeekæ„å»ºRAGç³»ç»Ÿ"""
    print("\nğŸ”¨ æ„å»ºDeepSeek RAGç³»ç»Ÿ...")
    
    try:
        from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
        from llama_index.llms.openai_like import OpenAILike
        
        print("âœ… å¯¼å…¥LlamaIndexåº“æˆåŠŸ")
        
        # é…ç½®DeepSeek-R1 æ¨ç†æ¨¡å‹
        print("ğŸš€ é…ç½®DeepSeek-R1 æ¨ç†æ¨¡å‹...")
        Settings.llm = OpenAILike(
            model="deepseek-reasoner",  # DeepSeek-R1 æ¨ç†æ¨¡å‹
            api_key=os.getenv('DEEPSEEK_API_KEY'),
            api_base="https://api.deepseek.com",
            is_chat_model=True,
            temperature=0.1,
            max_tokens=2000  # R1æ¨¡å‹éœ€è¦æ›´å¤štokensç”¨äºæ¨ç†è¿‡ç¨‹
        )
        print("âœ… DeepSeek-R1 æ¨ç†æ¨¡å‹é…ç½®å®Œæˆ")
        
        # é…ç½®æœ¬åœ°åµŒå…¥æ¨¡å‹
        print("ğŸ“¥ åŠ è½½æœ¬åœ°ä¸­æ–‡åµŒå…¥æ¨¡å‹...")
        Settings.embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-small-zh-v1.5",
            cache_folder="./models"
        )
        print("âœ… æœ¬åœ°åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åŠ è½½æ–‡æ¡£
        print("ğŸ“š åŠ è½½é£Ÿå“ç§‘ç ”æ–‡æ¡£...")
        documents = SimpleDirectoryReader(
            "food_research_data",
            recursive=True
        ).load_data()
        print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        
        # æ„å»ºç´¢å¼•
        print("ğŸ” æ„å»ºå‘é‡ç´¢å¼•...")
        index = VectorStoreIndex.from_documents(documents)
        print("âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ")
        
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        query_engine = index.as_query_engine(
            similarity_top_k=3,
            response_mode="compact"
        )
        print("âœ… æŸ¥è¯¢å¼•æ“åˆ›å»ºå®Œæˆ")
        
        return query_engine
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·å®‰è£…: pip install llama-index-llms-openai-like")
        return None
    except Exception as e:
        print(f"âŒ æ„å»ºå¤±è´¥: {e}")
        return None

def test_rag_system(query_engine):
    """æµ‹è¯•RAGç³»ç»Ÿ"""
    print("\nğŸ§ª æµ‹è¯•é£Ÿå“AIç§‘ç ”é—®ç­”ç³»ç»Ÿ...")
    
    test_questions = [
        "é£Ÿå“ä¸­æœ‰å“ªäº›ä¸»è¦çš„å¤©ç„¶æŠ—æ°§åŒ–å‰‚ï¼Ÿ",
        "é£Ÿå“å®‰å…¨æ£€æµ‹ä¸»è¦æ£€æµ‹å“ªäº›é¡¹ç›®ï¼Ÿ",
        "è‹¹æœå’Œæ©™å­çš„è¥å…»æˆåˆ†æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ¤” é—®é¢˜ {i}: {question}")
        print("-" * 40)
        
        try:
            response = query_engine.query(question)
            print(f"ğŸ¤– å›ç­”: {response}")
            
            if hasattr(response, 'source_nodes') and response.source_nodes:
                print("\nğŸ“š å‚è€ƒæ–‡æ¡£:")
                for j, node in enumerate(response.source_nodes, 1):
                    filename = node.metadata.get('file_name', 'æœªçŸ¥æ–‡æ¡£')
                    print(f"   {j}. {filename}")
                    
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        
        print("=" * 60)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ DeepSeek + æœ¬åœ°åµŒå…¥ RAGç³»ç»Ÿ")
    print("=" * 50)
    
    # æµ‹è¯•DeepSeek API
    if not test_deepseek_api():
        print("\nâŒ DeepSeek APIæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    # æ„å»ºRAGç³»ç»Ÿ
    query_engine = build_rag_with_deepseek()
    if not query_engine:
        return
    
    # æµ‹è¯•ç³»ç»Ÿ
    test_rag_system(query_engine)
    
    print("\nğŸ‰ ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()
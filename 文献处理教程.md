# LlamaIndex 文献处理完整教程

## 目录
1. 文献上传流程
2. 向量化原理
3. 实际操作示例
4. 高级功能

## 第一部分：文献上传流程

### 1.1 当前系统的文献加载方式

在你的系统中，文献加载使用 `SimpleDirectoryReader`：

```python
from llama_index.core import SimpleDirectoryReader

# 加载文档
documents = SimpleDirectoryReader(
    "food_research_data",  # 文档目录
    recursive=True         # 递归读取子目录
).load_data()
```

### 1.2 支持的文件格式

LlamaIndex 自动识别以下格式：
- PDF (.pdf)
- Word (.docx, .doc)
- 文本文件 (.txt)
- Markdown (.md)
- HTML (.html)
- CSV (.csv)
- JSON (.json)

### 1.3 文献上传的三种方式

#### 方式1：直接放入目录（当前使用）
```bash
# 将文件放入指定目录
cp your_paper.pdf food_research_data/papers/
cp your_data.txt food_research_data/datasets/

# 重启系统，自动加载
python app.py
```

#### 方式2：程序化上传
```python
import shutil
import os

def upload_document(file_path, target_dir="food_research_data/papers"):
    """上传单个文档"""
    # 确保目标目录存在
    os.makedirs(target_dir, exist_ok=True)
    
    # 复制文件
    filename = os.path.basename(file_path)
    target_path = os.path.join(target_dir, filename)
    shutil.copy(file_path, target_path)
    
    print(f"文档已上传: {filename}")
    return target_path
```

#### 方式3：Web界面上传（需要实现）
```python
from flask import request
from werkzeug.utils import secure_filename

@app.route('/api/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': '没有文件'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': '文件名为空'}), 400
    
    # 保存文件
    filename = secure_filename(file.filename)
    filepath = os.path.join('food_research_data/papers', filename)
    file.save(filepath)
    
    # 重新加载索引
    reload_index()
    
    return jsonify({'success': True, 'filename': filename})
```

## 第二部分：向量化原理

### 2.1 什么是向量化？

向量化是将文本转换为数字向量的过程：

```
文本: "苹果富含维生素C"
     ↓ (嵌入模型)
向量: [0.23, -0.45, 0.67, ..., 0.12]  (512维)
```

### 2.2 向量化的完整流程

```
原始文档 (PDF/TXT)
    ↓
【步骤1】文档解析
    ↓
纯文本内容
    ↓
【步骤2】文本分块 (Chunking)
    ↓
多个文本块 (Chunks)
    ↓
【步骤3】向量嵌入 (Embedding)
    ↓
向量数组
    ↓
【步骤4】存储到向量数据库
    ↓
可检索的向量索引
```


### 2.3 详细的向量化过程

#### 步骤1：文档解析
```python
from llama_index.core import SimpleDirectoryReader

# 读取文档
documents = SimpleDirectoryReader("food_research_data").load_data()

# 每个document包含：
# - text: 文档内容
# - metadata: 元数据（文件名、路径等）
for doc in documents:
    print(f"文件: {doc.metadata['file_name']}")
    print(f"内容长度: {len(doc.text)} 字符")
```

#### 步骤2：文本分块
```python
from llama_index.core.node_parser import SimpleNodeParser

# 创建分块器
parser = SimpleNodeParser.from_defaults(
    chunk_size=512,        # 每块512个token
    chunk_overlap=50       # 块之间重叠50个token
)

# 分块
nodes = parser.get_nodes_from_documents(documents)

# 示例输出
print(f"原始文档数: {len(documents)}")
print(f"分块后节点数: {len(nodes)}")
# 输出: 原始文档数: 3
#      分块后节点数: 45
```

**为什么要分块？**
- 文档太长，无法一次性处理
- 提高检索精度
- 控制上下文长度

#### 步骤3：向量嵌入
```python
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# 创建嵌入模型
embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-zh-v1.5"
)

# 对单个文本进行嵌入
text = "食品中的抗氧化剂"
embedding = embed_model.get_text_embedding(text)

print(f"文本: {text}")
print(f"向量维度: {len(embedding)}")  # 512
print(f"向量示例: {embedding[:5]}")   # [0.23, -0.45, ...]
```

#### 步骤4：构建向量索引
```python
from llama_index.core import VectorStoreIndex

# 自动完成：分块 + 嵌入 + 存储
index = VectorStoreIndex.from_documents(documents)

# 内部流程：
# 1. 将documents分块成nodes
# 2. 对每个node的文本进行嵌入
# 3. 将向量和文本存储到向量数据库
# 4. 创建索引以便快速检索
```

### 2.4 向量化的实际例子

假设有一个文档：

```
文档: 抗氧化剂研究.txt
内容: "维生素E是脂溶性抗氧化剂，主要存在于植物油和坚果中..."
长度: 2000字符
```

**处理过程：**

```python
# 1. 读取文档
doc = SimpleDirectoryReader("food_research_data").load_data()[0]
# doc.text = "维生素E是脂溶性抗氧化剂..."

# 2. 分块（假设每块200字符）
chunks = [
    "维生素E是脂溶性抗氧化剂，主要存在于植物油和坚果中...",
    "维生素C是水溶性抗氧化剂，广泛存在于新鲜水果蔬菜中...",
    "多酚类化合物包括花青素、儿茶素、槲皮素等..."
]
# 共10个chunks

# 3. 向量化每个chunk
embeddings = []
for chunk in chunks:
    vector = embed_model.get_text_embedding(chunk)
    embeddings.append(vector)
    
# embeddings = [
#     [0.23, -0.45, 0.67, ...],  # chunk1的向量
#     [0.18, -0.52, 0.71, ...],  # chunk2的向量
#     [0.31, -0.38, 0.59, ...]   # chunk3的向量
# ]

# 4. 存储到向量数据库
# 每个向量关联原始文本和元数据
```

## 第三部分：实际操作示例

### 3.1 手动上传文献

```bash
# 创建测试文档
echo "这是一篇关于食品安全的研究论文..." > test_paper.txt

# 上传到系统
cp test_paper.txt food_research_data/papers/

# 查看文件
ls -la food_research_data/papers/
```

### 3.2 查看向量化过程

创建一个测试脚本：


```python
# 运行演示脚本
python notebooks/06_文献向量化演示.py
```

### 3.3 实现文件上传API

在 `app.py` 中添加上传功能：

```python
from werkzeug.utils import secure_filename
import os

UPLOAD_FOLDER = 'food_research_data/papers'
ALLOWED_EXTENSIONS = {'txt', 'pdf', 'doc', 'docx', 'md'}

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/api/upload', methods=['POST'])
def upload_document():
    """上传文档API"""
    if 'file' not in request.files:
        return jsonify({'success': False, 'error': '没有文件'}), 400
    
    file = request.files['file']
    
    if file.filename == '':
        return jsonify({'success': False, 'error': '文件名为空'}), 400
    
    if not allowed_file(file.filename):
        return jsonify({'success': False, 'error': '不支持的文件格式'}), 400
    
    # 保存文件
    filename = secure_filename(file.filename)
    filepath = os.path.join(UPLOAD_FOLDER, filename)
    file.save(filepath)
    
    # 重新加载索引
    try:
        reload_documents()
        return jsonify({
            'success': True,
            'filename': filename,
            'message': '文档上传成功并已索引'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'索引失败: {str(e)}'
        }), 500

def reload_documents():
    """重新加载文档并构建索引"""
    global rag_engine, documents_count
    
    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
    
    # 重新加载文档
    documents = SimpleDirectoryReader(
        "food_research_data",
        recursive=True
    ).load_data()
    
    documents_count = len(documents)
    
    # 重建索引
    index = VectorStoreIndex.from_documents(documents)
    rag_engine = index.as_query_engine(
        similarity_top_k=3,
        response_mode="compact"
    )
    
    print(f"[成功] 重新加载了 {documents_count} 个文档")
```

## 第四部分：高级功能

### 4.1 自定义分块策略

```python
from llama_index.core.node_parser import SentenceSplitter

# 按句子分块
splitter = SentenceSplitter(
    chunk_size=1024,
    chunk_overlap=20
)

nodes = splitter.get_nodes_from_documents(documents)
```

### 4.2 元数据提取

```python
from llama_index.core.extractors import (
    TitleExtractor,
    QuestionsAnsweredExtractor,
)

# 自动提取标题和问题
extractors = [
    TitleExtractor(nodes=5),
    QuestionsAnsweredExtractor(questions=3),
]

# 应用提取器
from llama_index.core.ingestion import IngestionPipeline

pipeline = IngestionPipeline(
    transformations=[
        splitter,
        *extractors,
        embed_model,
    ]
)

nodes = pipeline.run(documents=documents)
```

### 4.3 持久化存储

```python
# 保存索引到磁盘
index.storage_context.persist(persist_dir="./storage")

# 从磁盘加载
from llama_index.core import StorageContext, load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
```

### 4.4 使用外部向量数据库

```python
# 使用Chroma向量数据库
import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore

# 创建Chroma客户端
chroma_client = chromadb.PersistentClient(path="./chroma_db")
chroma_collection = chroma_client.create_collection("food_research")

# 创建向量存储
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

# 创建存储上下文
from llama_index.core import StorageContext

storage_context = StorageContext.from_defaults(vector_store=vector_store)

# 构建索引
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context
)
```

## 第五部分：常见问题

### Q1: 如何查看已加载的文档？

```python
from llama_index.core import SimpleDirectoryReader

documents = SimpleDirectoryReader("food_research_data").load_data()

for doc in documents:
    print(f"文件: {doc.metadata['file_name']}")
    print(f"大小: {len(doc.text)} 字符")
```

### Q2: 如何删除某个文档？

```python
import os

# 删除文件
os.remove("food_research_data/papers/old_paper.pdf")

# 重新构建索引
reload_documents()
```

### Q3: 向量化需要多长时间？

- 小文档 (< 10页): 1-5秒
- 中等文档 (10-50页): 5-30秒
- 大文档 (> 50页): 30秒-2分钟

取决于：
- 文档大小
- 分块数量
- 嵌入模型速度
- 硬件性能

### Q4: 如何优化检索质量？

```python
# 1. 调整检索数量
query_engine = index.as_query_engine(
    similarity_top_k=5  # 增加到5个结果
)

# 2. 使用混合检索
from llama_index.core.retrievers import VectorIndexRetriever

retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=10,
)

# 3. 添加重排序
from llama_index.core.postprocessor import SimilarityPostprocessor

query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[
        SimilarityPostprocessor(similarity_cutoff=0.7)
    ]
)
```

### Q5: 如何处理大量文档？

```python
# 批量处理
from llama_index.core import VectorStoreIndex
from llama_index.core.ingestion import IngestionPipeline

# 创建处理管道
pipeline = IngestionPipeline(
    transformations=[
        splitter,
        embed_model,
    ]
)

# 批量处理文档
all_nodes = []
batch_size = 10

for i in range(0, len(documents), batch_size):
    batch = documents[i:i+batch_size]
    nodes = pipeline.run(documents=batch)
    all_nodes.extend(nodes)
    print(f"处理进度: {i+len(batch)}/{len(documents)}")

# 构建索引
index = VectorStoreIndex(all_nodes)
```

## 总结

### 关键流程

```
上传文档 → 解析文本 → 分块 → 向量化 → 存储 → 检索
```

### 核心代码

```python
# 1. 加载文档
documents = SimpleDirectoryReader("food_research_data").load_data()

# 2. 构建索引（自动完成分块和向量化）
index = VectorStoreIndex.from_documents(documents)

# 3. 创建查询引擎
query_engine = index.as_query_engine()

# 4. 查询
response = query_engine.query("你的问题")
```

### 实践建议

1. **文档组织**: 按类别分目录存放
2. **文件命名**: 使用有意义的文件名
3. **定期更新**: 新增文档后重建索引
4. **监控性能**: 关注索引构建时间
5. **备份数据**: 定期备份向量索引

---

**开始实践！**

运行演示脚本查看完整过程：
```bash
python notebooks/06_文献向量化演示.py
```
